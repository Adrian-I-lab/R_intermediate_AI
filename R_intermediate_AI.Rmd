---
title: 'Draft Plan Reproducible Analysis AI'
author: "Adrian Ilich"
date: "`r Sys.Date()`"
output:
 html_document:
   css: style.css
   code_folding: hide
   number_sections: true
   toc: true
   toc_float: true
   toc_depth: 3
---

# Reproducible Analysis and Good Coding practices in R

## Overview of workshop:

Idea: turn into a bioinformatics/statistician simulator. Run workshop as if we are starting a new project for a client, setting everything up, do some analysis, and do GitHub as we go to get some practice? Maybe end with new client and they give everything a go for themselves to actually solidify skills/info.

Day 1 part 1: Set up project, set up renv, set up github, open mark down/quarto file, install packages we need 

Day 1 part 2: Play around with our quarto/markdown to see which output and style we want our data report to have, insert dummy code e.g. from tutorial GitHub, render some things. Finish with pushing and committing to GitHub. 

Day 2 part 1: Pull from GitHub to get our project files again. Play with functions, lapply. Push and commit to github. 

*note* maybe we just run the first part like a normal tutorial but tell them at the start they will need to do everything themselves at the end when we give them a project?

Day 2 part 2: Try for yourselves, give them a scenario, dataset and goal e.g. I want to visualise my data and do some certain stats test to get the p-value. Start from scratch make a new project, renv packages, open a markdown file write the analysis, export the HTML output for your client.



## Step 1: set up your project and make sure things will be reproducible

To set up a project we need to make sure we do two crucial things at the start

1.  Make a new R project with renv to keep all things related to your project in one place

-   using renv when making your project ensures you don't have reproducibility issues down the line if packages get updated or if there is a new version of R. Once a package is installed into this new project, the package will be locked in the version it was installed as and can never be changed without altering the project. This is best practice as things update and change very fast in the bioinformatics/coding/stats space so when doing analysis it is basically guaranteed that any code you write today will come up with errors if you try to run it in a years time unless you make sure all packages and versions are the same!

2.  Make a github repository and git related to your project so you can keep safe backups

-   Git allows you to keep all versions of your scripts so if one week you realize you have ruined your analysis and all these random errors pop up all of a sudden, you can go onto github and pull your script from last week and not have to worry. It will also be your safeguard in case a catasrophy strikes such as you type "rm -rf" in the HPC and delete your whole project. The HPC is not backed up if you delete something its gone forever, so its incredibly important that you have back ups. These script versions are also your documentation and can be useful to you in the same way a lab book is useful to a wet lab researcher, you can go in to old projects or old script versions and see what you were doing, you can also see the exact day and time you uploaded something to github in case you ever need to show you completed certain work by a certain day/time.

(probably will have to have a mini git/github tutorial here)

Make diagram to explain: Code Project (git status, git add, git commit)--\> git file (git push)--\> github (git pull ) --\> get Code Project from anywhere --\> repeat cycle

Overtime you will have an in depth diary and documented journey of your code analysis which is 1) safe and useful for you but also 2) important for publishing or any kind of software/package development and 3) incredibly useful if there is ever any legal issues, this is your dry lab lab book.

Check list:

-   have R project folder with renv/ folder 

-   have git hub account

-   have git hub repository which is named the same as R project (to keep things easy to track)

-   have .git file in your R project

## Step 2: set up your project folders and files

Make folders to keep things neat. I like to organised the project base folder to have only script files then have a folder for raw data or inputs, a folder for outputs and results, and finally a folder for R_objects that you may want to save throughout your work (i.e. so you can start stop throughout your pipeline --\> never save environment image this leads to inconsistencies and ruins reproducibility of your code). Finally make an Rmarkdown file which we will use to write our analysis in.

We also need to make a git ignore file so we can make our lives easier when using git/github. Only upload scripts never data, images, etc. git ignore file is a file which is read every time you try to add something to your git file and if the folder, file name or file extension is listed in the git ignore file it will never be added to the git file and therefore never committed to the git file or pushed to github. Important as if you make everything open access it can have issues with client confidentiality, reduce trust or decrease interest if you want to commercialise a product or even reduce your appeal to high impact journals if your data and code is publicly available before you publish. Always keep github repository on private until you are sure you want to publish and remember everything you commit to that repository can be viewed by anyone once you make it public.

Talk with BD or legal department at QIMR if you are unsure or working on a commercial product that will be made available to learn more about specific github licenses you can apply to public repositories to protect your IP.

Checklist:

-   have Data/, Outputs/, and R_objects/ folders in your R project folder
-   have .gitignore in your project folder
-   have Rmarkdown file in your project folder

## Step 3: Link your github account

This can seem overwhelming and annoying at first but it is so important and useful. Set up has quite a few extra steps but once you set up your repository and project there are truly only 5 functions that you will use from day to day.



git status: what files in your project, if any, are changed or not added to your .git file 

git add .: add everything in your project folder to your .git (anything listed in .gitignore file is not added unless you forcefully add it e.g. git add -f path/to/file)

git commit -m "now figure 1.1 has black and white theme": now that things are added onto the git file we need to actually save them (or commit it) think about opening a word document and then closing it before you hit save, you loose all your work. Always commit changes once you add. Also always add a comment explaining the impact of your change. Its common practice to not comment what you did but the implications of the change i.e. 

"Changed ggplot on line 33 to have geom_bar as well as geom_freqpoly." bad comment :(

"Figure 1.1 now shows data frequency histogram line overlayed on bar plot." good comment :)

git push origin main: now that things are saved on your .git file lets back it up on github. The whole point is to have a nice back up and be able to share from any device so only having a .git is better than nothing but stopping there you lose a lot of good benefits of doing this. Also you are keeping your back up in the same folder as the actual thing so its not a good back up. git push moves it to your repository but you can have multiple branches (more advanced than we need to go into for this) but the main branch is called the main origin so you push to origin main.

Next time you log into a new computer without your files then you just do:
git pull origin main: get all the files from your github from the main branch

And those are the only fucntions you actually need to remember. However, as mentioned it will take a few extra steps to get things set up and linked from R to github.

### Git Step 0: Make sure accounts are set up and software installed

You should already have a github account made and a repository made with the same name as your project.
https://swcarpentry.github.io/git-novice/index.html#creating-a-github-account

You should also have git installed on your computer:
https://carpentries.github.io/workshop-template/install_instructions/#git

  for windows shortcut:
  https://gitforwindows.org/


### Git Step 1: 

open terminal and cd to your project folder. It should be already there but just check:

```{r}
getwd()
```

Now we need to configure settings for git such as our user and email.

If you use a private email address with GitHub, then you can use GitHub’s no-reply email address for the user.email value. It looks like ID+username@users.noreply.github.com. You can look up your own address in your GitHub email settings. Otherwise you can use your private email or your QIMR email too. It doesnt really matter.

```{bash, eval = F}
git config --global user.name "adrian ilich"
git config --global user.email "adrian.ilich@qimrb.edu.au"

# Windows:
git config --global core.autocrlf true

# MacOS and Linux:
# git config --global core.autocrlf input

# I also prefer to set default editor to nano
git config --global core.editor "nano -w"

# set default branch
git config --global init.defaultBranch main

# you can check your updated global git settings with: 
git config --list --global
```

these updated settings will stay default for all future git projects on this account/computer now.

### Git Step 2:

make sure you are in your R project folder. Then run the following command to create a git file for this folder:

```{bash, eval = F}
# Makes the .git file
git init
# Sets the branch we are on to main, which is already default but just in case
git branch -M main
```

### Git Step 3: set up github remote

make github repo if you haven't already (Should be empty no read me and no .gitignore)

in repo page click on green code button, in local under clone click ssh and copy the link into the command below


```{bash, eval = F}
# Add origin(or branch) which is actually just the link to the github repo page
git remote add origin git@github.com:Adrian-I-lab/R_intermediate_AI.git
# Check command worked, should show the github ssh name for fetch and push 
git remote -v
```
### Git Step 4: Set SSH pair (this is the password to link to your github)

Make SSH key pair. This is essentially an encrypted password so that only this comuter and session can actually connect to the github repository. Keeps things extra safe works for private and public repositories. In fact its always needed. 

Do the following step in the rstudio terminal as it asks for interaction/response which cant be done in R markdown chunk and just freezes your rstudio session. It will ask for file name, then password, then confirm password. Just press enter 3 times for default file name and no password. This will make an ssh key in your user folder on the computer in the secret .ssh/ folder:
e.g. C:/Users/AdrianI/.ssh/id_ed25519 


```{bash, eval = F}
# Interactive run in terminal
ssh-keygen -t ed25519 -C "adrian.ilich@qimrb.edu.au"
```

### Git Step 5: tell github the password  
We have an ssh key pair but we need to give one of the two files to github. We always give the public file not the private key. to get it run the following command and then cimply copy the output text:

```{bash, eval = F}
cat C:/Users/AdrianI/.ssh/id_ed25519.pub
# but change folder to your name
```
once you have copied the key go into github. Go into your github settings (NOT THE REPOSITORY SETTINGS) and go to SSH and GPG keys. click add new ssh key (top right corner green button) and then simply make a name that describes where this key is to e.g. Personal laptop key or HPC_access key etc. then paste the id_ed25519.pub file contents that you copied in the box below and save. Leave key type as authentication key. Github may ask you to enter a authentication code that it emails to your email as a safety precaution.

Now lets set up the link between our R project and github repository. RUn in terminal again as it also requires interaction. it will ask you are you sure you want to continue connecting, just type "yes".

```{bash, eval = F}
# Interactive run in terminal
ssh -T git@github.com
```


### Git Step 6: Push local changes to github

Now we have set up git and github and should be ready to go. As mentioned this set up is only done once, from now on you really only use those 5 key functions to push and pull from your files to github and back. Lets try:

```{bash, eval = F}
# Check what is changed in our files from our .git file
git status
```
No commits yet, lets try adding and committing. Make sure that there is nothig in the untracked fiels that shouldnt be there e.g. data, images, or our renv files....

Lets update our .gitignore file. Since we are in rstudio we can just open the file and edit it here. On the HPC you would need to use nano or vim to edit it. after updating the git igonore file mine looks like this. You may need to use your own file and folder names if you named things differently:

```
.Rproj.user
.Rhistory
.RData
.Ruserdata

renv/

renv.lock

Data/

Outputs/

R_objects/
```

Now none of these files or folders will ever be added to git/github. As your project progresses remeber to always check each time you go to add using git status if there is any kind of file or folder that shouldnt be there and upfdate this gitignore file accordingly.

Lets check again to make sure its all safe to commit now:
```{bash, eval = F}
git status
```

Everything looks okay so now lets add and commit:

```{bash, eval = F}
git add . # The dot means add everything in folder, if you want to add a specific file you can put file name here
```

remember now we have added it to the git file but its not saved (an open word document which you have not saved analogy). To save the added changes on the git file we need to commit, and every commit, since they are all saved and viewable forever, needs to have a comment explaining what the change is. Make the comment useful to someone who doesn't know what you are doing, or your future self whom forgot what this script was even for. Commenting is best done when you describe the change and effect and even sometimes reason why you did that. 

e.g. "line 27 changed" is a bad comment. "updated color scheme and font size on figure 1 bar plot" is better comment. "Updated figure 1 bar plot from set1 to set2 colour scheme to match grant requirements and increased font size of axis titles for visability" Is even better.

lets commit our changes. Initial commit comments usually arent that important since we havent done too much yet

```{bash, eval = F}
git commit -m "Initial commit. Made R project, git, and github"
```

Now this all saved on our git file and some people do stop here but its highly NOT recommended. Its much safer to push to github and keep a backup of everything there. I have accidently deleted an entire project folder on the HPC including the .git file which I hadnt pushed to github in months and essentially lost almsot half a year worth of scripts. Ever since then I push my git file at the end of every work day.

```{bash, eval = F}
git push -u origin main
```
Now you can go check your github page and you should see everything is there with your commit comments previewed as well.

## Step 4: Set up the YAML chunk for the Rmarkdown

Now we have our project and github set up, its time to start working. Lets start by making a new markdown file where we will write our code and keep all our analysis notes.

An R markdown file, unlike normal .R script file, can be rendered into a report e.g. pdf, docx or html files. The output and style settings of the doumetn are always mentioned at the top of the markdown in whats called a YAML chunk. In this chunk you give a bunch of parameters and settings that are employed when you render your document. Here are two examples, one for a HTML report and one for a PDF report.

Example HTML file YAML
```         
---
title: "Draft Data Report 1"
author: "Adrian Ilich"
date: "`r Sys.Date()`"
output:
 html_document:
   css: style_wide.css
   code_folding: hide
   number_sections: true
   toc: true
   toc_float: true
   toc_depth: 3
---
```

Example PDF file YAML
```         
---
title: "Draft Data Report 1"
author: "Adrian Ilich"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
---
```

### Rmarkdown YAML section notes:

You can add heaps of things in your YAML here we list many of the important and frequently used settings but there are even more than this out there.


*1. Document Metadata*

Beyond title, author, date, and output, you can also specify:

abstract: A summary that appears at the beginning of some output formats.

subtitle: Adds a secondary title under the main one.

keywords: Useful for indexing or academic articles.

lang: Language code (e.g., en-AU) for localisation.

bibliography: Path to .bib file for references.

csl: Citation style language file (e.g., APA, Nature).

link-citations: true/false to hyperlink in-text citations to the bibliography.


*2. Output Customisation*

Each output format can have nested options. For example:

output:
  html_document:
    toc: true
    toc_float: true
    theme: journal
    highlight: tango
  pdf_document:
    latex_engine: xelatex
    number_sections: true


*3. Code & Chunk Options*

These affect how code and results are shown:

echo: Show/hide code.

eval: Run or skip code.

warning / message: Display or suppress warnings/messages.

error: Show errors instead of stopping render.

code_download: Provide a download link for source code.


*4. Figure & Table Options*

fig_width / fig_height: Control figure size.

fig_caption: Allow figure captions.

fig_align: Left, centre, right.

df_print: Control table printing style (e.g., kable, paged).


*5. Navigation & Structure*

toc_depth: Depth of TOC expansion.

number_sections: Adds section numbering.

anchor_sections: Adds anchors to headings for linking.

linkcolor / urlcolor (LaTeX): Style hyperlinks.


*6. Interactivity (HTML only)*

code_folding: "show", "hide", or "none".

code_download: Enable download of source .R script.

toc_float: Floating table of contents.

self_contained: true/false (embed or link assets).

mathjax: For rendering equations.


*7. Advanced / Package-Specific*

runtime: For interactive execution (e.g., shiny, shiny_prerendered).

params: Define parameters for parameterised reports (useful for pipelines).

includes: Add extra files like headers/footers (HTML or LaTeX).

keep_tex: Keep the intermediate TeX file for debugging.

citation_package: natbib or biblatex (for LaTeX).

## Step 5: Importing packages and loading libraries 

In the R markdown now that we have made the YAML lets start populating the document

Normally I follow this template roughly

```{r, eval = F}
# Introduction
## Experiment/data
## Research QUestions/aims

# Packages

# Load in data

# Analysis
## step 1
## Step 2
### Step 2.1 
# etc...

# conclusions/findings summary

# session info
```


### Packge version control (and renv)

It is important to keep track of, and report, the versions of R and R packages you used for analysis.

We have made the project using renv so any installations will be automatically locked for the project. Be aware of compatibility issues certain packages may only work with certain versions of R and other packages versions of dependent packages. Try not to install from two different versions of bioconductor for example... 

Describing your R environment when things were calculated is required for reproducibility. If all you need is the version of R you are using, the built in object `version` is a list of different character strings that include the R version number, date of release, and platform you are using R on (among other things). A more simple way to get the R version is the `getRversion()` function, and you can use it's return value to make comparisons (in case you want your code to do different things based on the version number). Finally, the `sessionInfo()` function returns more information about the R session at the time of running the function, including the version numbers of the loaded packages.

-   Session Info and version

```{r Session info, eval = F}
# Just R version 
getRversion()
# comparisons
getRversion() >= "4.3.0"
getRversion() >= "4.4.0"

# more details about the R version. Note this is not a function
version

# details about your R session
sessionInfo()
```

-   \^ already mentioned in existing workshops\*

### Package version control (without renv)

The R package system is designed to override existing versions of packages, with the aim that the user always has the latest version. *If you want to have multiple versions of a package on your device* then you will need to download them in different locations.

You are likely already familiar with install packages using the `install.packages()` function. If you don't specify the `lib` argument, the default library location is used, with is the the first element (if there are multiple elements) of `.libPaths()`. You can change you library pathways, or install a package specifically at different library location, e.g. `install.packages('foo', lib = 'C:/User/Me/R/Library/Test')`.

If you want to install an older version of a package, the `install_version()` function from the `remotes` package has this capability. The `remotes` package also has function designed to install packages from locations besides CRAN, including in devleopment versions from github.

```{r package version}
#| eval: false

# to install a package
install.packages("dplyr")

# to install a package at a specific location
install.packages("dplyr", lib = "C:/R/Libraries/Test")

# to install a specific version
library(remotes)
remotes::install_version("dplyr", version = "1.0.7")
```

The Packages pane in RStudio lists the packages downloaded to your system, including a description, the version number, and a checkbox for if the have been libraries in to the session. this information can similarly be accessed through the function `installed.packages()`, which includes where the packages has been downloaded, dependencies, and other information. We recomment viewing the output from installed.packages()`through the`View()\` function in RStudio

```{r view install packages}
#| eval: false

# view installed packages
View(installed.packages())

```

### Package version control with renv

Initiate renv:

```{r, eval = FALSE}
renv::init()
```


*side note*: if you are at QIMR your computer will likely have internet access issues in R studio due to the QIMR firewalls. To use renv you need to first install curl and stringr on your computer from CRAN. The you can initiate renv and transfer the installed packages from your computer to the renv library folder using hydrate. once those are installed on renv you can run Satomi's Proxy_Settings_R.R script to be able to connect to the internet. IF you get errors with the file try maually altering the script to have your name as first.last. If you still get an error try typing in your password already URL encoded. example code below:

```{r, eval = F}
# How to get around QIMR proxy issues in R Studio
install.packages("curl")
install.packages("stringr")
renv::init()

# Use hydrate to get packages that were already in your library on your computer e.g. on work computer proxy blocks renv from installing so you need to hydrate curl and Stringr before running Satomi's proxy
renv::hydrate(packages = 'curl',sources = 'C:/Program Files/R/R-4.4.1/library')
renv::hydrate(packages = 'stringr',sources = 'C:/Program Files/R/R-4.4.1/library')

# Run proxy script. Email us if you do not have it.
source("path/to/Proxy_Settings_R.R")
```

From there renv should work as normal and be able to install any bioconductor or CRAN package with renv::install("XXX")

If you are installing packages and renv.lock pins package X at version 1.2.3 but you want 1.3.0 you can Install specific versions explicitly:

```{r, eval = FALSE}
renv::install("packageX@1.3.0")
```

Then update the lockfile with:

```{r, eval = FALSE}
renv::snapshot()
```


This will replace the pinned version. However, this is not recommended maybe only do this during inital installations of packages but once you have started some analysis dont do this as changing versions may result in your code no longer working or having compatibility issues.

To deactivate renv in a project, use `renv::deactivate()`. This removes the renv auto-loader from the project .Rprofile, but doesn’t touch any other renv files used in the project. If you’d like to later re-activate renv, you can do so with `renv::activate()`.

To completely remove renv from a project, call `renv::deactivate(clean = TRUE)`. If you later want to use renv for this project, you’ll need to start from scratch with `renv::init()`.

### Package and software version control 

If you use other software than R, e.g. you can use reticulated to write in bash, python, and R all in the same R markdown document. For this renv doesn't work anymore as it only covers the R packages. When things get more complicated you will need to start using and building containers. This is out of the scope of what we do here but its good for you all to know this exists and they are essentially just renv but for everything and anything so you can control python, r and bash based packages and libraries all at once. 

*Checklist:*

- Have Rproject, with all folders set up, and linked to git and github
- Have set up an R markdown file to work in
- Initiated renv to control package versions in your project

Now we are ready to start coding!

## Step 6: Learning some more advanced coding skills

### Functions

Just like we use functions from different packages to make a change to the environment or some input, we can write our own functions. This can be particular useful when we identify large sections of code that are repeated (i.e. copied, pasted, than changed slightly). By turning the repetitious code into a function, we can make our code tidier and easier to read, as well as reducing the risk of the code being incorrectly copied or updated.

In the chunk of R code below, we create a function that calculates the length of the hypotenuse in a right-angled triangle (via the Pythagorean theorem, $c^2=a^2+b^2$ or $c=\sqrt(a^2+b^2)$). Firstly, we assign the function to an object called `pyth_func`. Inside the `function()` parentheses, we include what our function arguments are going to be. We can also decide to give the arguments default values (here we say `a` is 3 by default and `b` is 4 by default). Whenever we call the function `pyth_func()`, we can provide values for `a` and `b` (or use the default values). Inside the curly braces `{}` after the closing of the parenthesis, we include the code we want the function to perform each time we call it. In the case here, the function `pyth_func()` makes two objects (`a_sqrd` which is `a` squared, and `b_sqrd` which is `b` squared), then calculates the square root of the sum of those objects, the result of which is assigned to the object `c`. The object `c` is then the argument of the `return()` function, which is how the function `pyth_func()` knows what to output when it is called (i.e. the output of `pyth_func()` is `c`).

While the first three lines of code in the `pyth_func()` function could all be done in one line (i.e. `c <- sqrt(a^2 + b^2)`), using the function as is highlights an important point: if something isn't printed to the screen or given as input to the `return()` function, then it will no longer exist once the main function has finished running. In the case of the `pyth_func()` function, once it has finished running the `a_sqrd` and `b_sqrd` objects won't exist in your environment.

```{r basics of a function}
#| eval: false

# Pythagorean theorem, c^2=a^2 + b^2
pyth_func <- function(a = 3, b = 4){
  a_sqrd <- a^2
  b_sqrd <- b^2
  c <- sqrt(a_sqrd + b_sqrd)
  return(c)
}

```

You don't strictly *have* to use the `return()` function. If there is no `return()` function, the last thing that was printed to the console will be the output.

Note also, you can only have one object as an argument in the `return()` function to be the output of your function. If your user written function creates many objects you want to have as output, this can be achieved by making a list with all the output as elements of that list.

See the example below. The function `summarise_billboard()` summarizes a data set of Billboard top 100 hits, calculating multiple things of interest. The object that is returned from `summarise_billboard()` is a list object that contains: the number of songs in the charts that year, a collection of artists that had multiple hits, the tracks that reached number 1 on the charts, and how long each track was on the charts. This function could be used for Billboard top 100 data from any year (provided the data is provided in the same format).

```{r more complex function}
# eval: false
library(tidyverse)

df <- billboard # song rankings on Billboards top 100 in the year 2000

summarise_billboard <- function(data){
  # summarise_billboard takes the billboard chart information from any year, and
  # calculates summarises of the data
  
  # input: data is a dataframe with columns:
  #           $ artist        [name of artist]
  #           $ track         [name of track]
  #           $ date.entered  [date entering the charts]
  #           $ wk1           [ranking in first week on charts]
  #           $ ...           [wk2, wk3, wk4, etc.]
  
  # output: list of 4 elements
  #           $ Number of Songs  -  integer number of songs in charts
  #           $ Common Artists   -  tibble dataframe with variable artist. 
  #                                 Contains only artists with at least 3 songs 
  #                                 in charts
  #           $ Number 1s        -  tibble dataframe with variables artist and
  #                                 track. Contains tracks that were number 1 
  #                                 on the charts.
  #           $ Chart Length     -  tibble dataframe with variables artist, track
  #                                 date.entered, and num_weeks. The variable 
  #                                 num_weeks is the number of weeks the track 
  #                                 was in the charts.
  
  # number of songs on the chart
  num_songs <- dim(data)[2]
  
  # artists with 3 or more songs in charts
  common_artists <- data %>% 
    group_by(artist) %>% 
    mutate(num = n()) %>%
    filter(num >= 3) %>%
    ungroup() %>%
    select(artist) %>%
    distinct()
  
  # artists with a number 1 and what track was number 1
  number_ones <- data %>% 
    pivot_longer(cols = -c("artist","track","date.entered"), names_to = "week", values_to = "place") %>%
    filter(place == 1) %>%
    select(artist, track) %>%
    distinct()
  
  # weeks in charts
  chart_weeks <- data %>%
    pivot_longer(cols = -c("artist","track","date.entered"), names_to = "week", values_to = "place") %>%
    drop_na() %>%
    group_by(artist, track, date.entered) %>%
    summarise(num_weeks = n()) %>%
    ungroup()
  
  # combine output into one list object
  billboard_output <- list(`Number of Songs` = num_songs,
                           `Common Artists` = common_artists,
                           `Number 1s` = number_ones,
                           `Chart Length` = chart_weeks)
  
  # return output
  return(billboard_output)
}

# use function
billboard2000 <- summarise_billboard(df)

# number of songs
billboard2000$`Number of Songs`

# common artists
head(billboard2000$`Common Artists`)

# number 1s
head(billboard2000$`Number 1s`)

# chart length
head(billboard2000$`Chart Length`)

```

*Some material already exists from earlier workshops*

### Functions with lapply

Sometimes you may have many samples that need to first be cleaned or processed before you can integrate the results or compare between groups. This means a lot of the times there is a large chunk of repeated code for each sample which could be condensed and cleaner with using a loop. Particularly in bioinformatics where sequencing data for example needs to be standardized for number of cells or sequence reads per sample first before multiple samples can be compared with each other. Here is an example from a single cell RNA sequencing experiment.

This experiment works by getting individual cells, breaking them open, taking all of the RNA from that individual cell and barcoding those RNA pieces so we know which cell they came from. Then we put all the RNA in a sequencer which reads the RNA as well as the barcode and constructs a matrix for each sample with all the cells as rows and all the columns as genes, in the cells of the matrix is the number of RNA sequences of a particular gene that were read in a particular cell.

In this imaginary experiment we have 10 samples and each had slightly variable number of cells but exactly 10 million reads per sample. In the sequencer in each well (one sample per well) the reads are distributed randomly meaning if we have variation in cell number in the sample (which we always do) then we also get variation in the average number of reads per cell. Thus, how can we accurately compare if a cell has more reads than another cell if they have differences in the total reads in the cell. Is it that a gene is being expressed more in once cell or was there simply overall more sequencing done in one cell. TO get around this we first do a bit of pre-processing and standardize reads before trying to compare between samples/

No actual data here just example from my code:

```{r, eval = FALSE}
# Sample 1
scRNAseq_sample_1 <- FindVariableFeatures(scRNAseq_sample_1, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_1 <- NormalizeData(scRNAseq_sample_1, 
                                   assay = "RNA")

scRNAseq_sample_1 <- ScaleData(scRNAseq_sample_1,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)

# Sample 2
scRNAseq_sample_2 <- FindVariableFeatures(scRNAseq_sample_2, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_2 <- NormalizeData(scRNAseq_sample_2, 
                                   assay = "RNA")

scRNAseq_sample_2 <- ScaleData(scRNAseq_sample_2,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)

# Sample 3
scRNAseq_sample_3 <- FindVariableFeatures(scRNAseq_sample_3, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_3 <- NormalizeData(scRNAseq_sample_3, 
                                   assay = "RNA")

scRNAseq_sample_3 <- ScaleData(scRNAseq_sample_3,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)

# Sample 4
scRNAseq_sample_4 <- FindVariableFeatures(scRNAseq_sample_4, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_4 <- NormalizeData(scRNAseq_sample_4, 
                                   assay = "RNA")

scRNAseq_sample_4 <- ScaleData(scRNAseq_sample_4,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)

# Sample 5
scRNAseq_sample_5 <- FindVariableFeatures(scRNAseq_sample_5, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_5 <- NormalizeData(scRNAseq_sample_5, 
                                   assay = "RNA")

scRNAseq_sample_5 <- ScaleData(scRNAseq_sample_5,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)

# Sample 6
scRNAseq_sample_6 <- FindVariableFeatures(scRNAseq_sample_6, 
                                          assay = "RNA", 
                                          verbose = F)

scRNAseq_sample_6 <- NormalizeData(scRNAseq_sample_6, 
                                   assay = "RNA")

scRNAseq_sample_6 <- ScaleData(scRNAseq_sample_6,
                               assay = "RNA",
                               vars.to.regress = c("nFeature_RNA", "percent.mt"),
                               verbose = F)
  
```

This work flow can be simplified using a list object and loops. This doesn't however, really change the run time as each sample is still processed sequentially. List apply, lapply(), just loops over each item in the list object and runs a fucntion on each object. This can be a function from base R or a loaded package or you can make your own!

```{r, eval = FALSE}
lapply(X = list_object, function(x){ 
  #Insert function
  }
)
```


```{r, eval = FALSE}
scRNAseq_list <- list("Sample 1" = scRNAseq_sample_1,
                      "Sample 2" = scRNAseq_sample_2,
                      "Sample 3" = scRNAseq_sample_3,
                      "Sample 4" = scRNAseq_sample_4,
                      "Sample 5" = scRNAseq_sample_5,
                      "Sample 6" = scRNAseq_sample_6,)

scRNAseq_list <- lapply(
 X = scRNAseq_list,
 FUN = function(x) {
 
 x <- FindVariableFeatures(x, assay = "RNA", verbose = F)
 
 x <- NormalizeData(x, assay = "RNA")
 
 x <- ScaleData(x, assay = "RNA",
                vars.to.regress = c("nFeature_RNA", "percent.mt"),
                verbose = F)
 return(x)
 }
)
```

It is much eaasier and cleaner to read this lapply() code compared to the expanded example above. In more advanced applications this can be configured to be run in tandem using parallel computing but that is out of the scope of this workshop for now. Extra reading about "future" package if you want to learn more.

### Data Manipulation

Sometimes your data isn't in the right format for an analysis or a function. Copy-pasting your data into a different format, or double-entry, can lead to accidental data errors. It can be safer to use code to reformat or manipulate your data, as it will do this systematically and you can also use that script file or function as a record of what you did to your data.

You will of course need to check that your code is performing as you desire (i.e. check you haven't made a mistake in coding). It can be useful to leave a version of your original data untouched, and make all manipulations on a *working* version.

Here we will go over some of the ways you can manipulate your data (focusing on using data wrangling verbs from the tidyverse).

Load tidyverse & dataset:

```{r}
library(tidyverse)
data("starwars")  # built-in dataset, fun and messy
```

*Core verbs*
Introduce select(), filter(), mutate().

select(): choose variables
filter(): subset rows
mutate(): create new variables

```{r}
starwars %>% select(name, height, mass)
```

```{r}
starwars %>% filter(species == "Human", gender == "masculine")
```

```{r}
starwars %>% mutate(bmi = mass / (height/100)^2)
```

This is how you’d extract only the relevant patient variables, filter out missing data, and calculate new clinical measures like BMI. They are the core verbs of data wrangling you will likely use a lot.

*Summarising*

Introduce group_by() + summarise().

```{r}
starwars %>%
  filter(!is.na(height), !is.na(mass)) %>%
  mutate(bmi = mass / (height/100)^2) %>%
  group_by(gender) %>%
  summarise(mean_bmi = mean(bmi, na.rm = TRUE),
            n = n())
```


The function `count()` is shorthand for frequencies. E.g. This is like giving the average height per demographic group in a survey.

*Reshaping*

Introduce pivot_longer() and pivot_wider(). starwars wasn't good for this so lets made a dummy data frame.

```{r}
df <- tibble(id = 1:2, glucose_day1 = c(85, 90), glucose_day2 = c(100, 110))

df %>%
  pivot_longer(cols = starts_with("glucose"),
               names_to = "day",
               values_to = "glucose")
```

normally wide to long is better for tidy formats and ggplot, but you might want to go long to wide for certain functions or when printing data tables for reports.

Other useful functions are: `rename()`, and `arrange()`.

```{r}
# Original names
starwars %>% select(name, birth_year, homeworld) %>% head()

# Rename columns
starwars %>%
  rename(
    person = name,       # new_name = old_name
    yob    = birth_year
  ) %>%
  select(person, yob, homeworld) %>%
  head()
```

This is like renaming patient_id → id or bp_sys → systolic to make your dataset clearer before analysis.

```{r}
# Sort characters by height
starwars %>%
  select(name, height, mass) %>%
  arrange(height) %>%   # ascending order
  head()

# Sort by mass in descending order
starwars %>%
  select(name, height, mass) %>%
  arrange(desc(mass)) %>%
  head()
```

This is like arranging patients by age or glucose level to quickly spot extremes or to prepare a clean table for a report. Also very commonly used in bioinformatics if for example you want to list the top differentially expressed genes between your treatment and control groups.

### Activity: test your tidyverse skills

1. Make a new df based on starwars with only name, species, homeworld, by using `Select()`.

2. Filter starwars to print only humans from Tatooine and in order of mass values from heaviest to lightest.

3. Calculate average height per species in the starwars df.

### Extracting subsets from you data

Data set input, manipulating form for different purposes (not need multiple copies to exist) different packages/functions need different formats, how to reformat for different purposes

*Some material already exists from earlier workshops*

### Modular Code

modular coding refers to making sections or even individual scripts to run parts of the overall pipeline. Each of these sections or scripts can be sequenced together to form the full pipeline. It might seem like extra work than just running everything from start to finish but for large piplines its incredibly useful as you know e.g. an error came from script 4 instead of you know there was some kind of error probably somewhere in the middle of you 10,000 line script of code...

Also you can update and rearrange certain modules or start and stop your pipeline in different sections. This can be useful if e.g. one function in your pipeline take 500gb ram and needs parallel computing while the other 10 scripts/modules in your pipeline only need 32gb ran and one core. Running this in modules means you can free up space on the HPC and only request what you need for each section of your pipeline.

All you really need to do to make your analysis pipeline modular is adding saveRDS and readRDS checkpoints throughout your pipeline. e.g. I am analyzing single cell data:

raw sequencing data-->
Script 1: load raw data and do QC, save R_object1

R_object1-->
Script 2: load R_object1 do dimensionality reduction, save R_object2

R_object2-->
Script 3: load R_object2 do clustering analysis and naming, save R_object3

R_object3-->
Script 4: Load R_object3 do differential gene expression analysis and find enriched molecular pathways that are occurring between control and treatment groups (get DEGs and run KEGG enrichment), save R_object4

this way If I want to change my clustering or try adjust dimensionality reduction parameters I don't need to run everything from scratch, which can take hours, but instead load the last checkpoint and go from there. As long as you are using good practices, keeping same package versions using renv and consistently using git there should be no reproducibility issues with this and it will make your life so much easier.

In more advanced applications this can become very large with many modules and it can be helpful to have a better way of keeping track of these modules within the overall pipeline. This can be done with a pipeline manager such as snakemake. Outside of the scope of this workshop but additional reading here:
https://snakemake.readthedocs.io/en/stable/

# Step 7: Your assignment

Choose project: Statistics or Bioinformatics (single cell seq)

To do the data analysis you need to follow all the steps we learned today to complete and R based project. Then you need to send the report and results to your client. To summaries that means:

1. create and R project 
2. set up all your files and folders in the project
3. set up a git in your project
4. set up a github repository
5. link your Rporject git and github
5. Make your Rmarkdown document (set up the YAML chunk and headings)
6. Set up renv and install needed packages
7. Follow the data analysis steps for which ever project you chose
8. render your report
9. add and commit to git and push to your github
10. send your github and data report to your client

## Bioinformatics Project: Find cell populations in pbmcs

Whole blood (PBMC = Peripheral Blood Mononuclear Cell) samples were taken from a human patient and 2700 cells were sequenced using scRNAseq 10X technology. First lets load data in, it was downloaded from Seurat website: PBMC 3k guided tutorial vignette. (https://satijalab.org/seurat/articles/pbmc3k_tutorial)

```{r}
library(Seurat)
# load the data in:
pbmc.data <- Read10X(data.dir = "C:/Users/AdrianI/Downloads/pbmc3k_filtered_gene_bc_matrices.tar/pbmc3k_filtered_gene_bc_matrices/filtered_gene_bc_matrices/hg19/")
```

The Seurat package has its own R object type which stores the counts matrix in one place and the metadata for each cell in another place. Think of it like a special and more complicated list object. Now we have the counts matrix data (i.e cells as columns and genes as rows, RNA counts per gene per cell as data in the matrix cells) we can use it to create our Seurat object. All future Seurat functions work from/on this object!

```{r}
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
pbmc
```

Quality control steps, we need to only keep good quality cells that represent the real population in vivo. Unfortunately, the experiment to get the sequencing data is quite rough and not perfect so you always end up with some dead/dying cells and also technical artifacts or ambient contamination RNA. TO deal with this we remove low quality cells based on filter cut off values. If a cell has more than 5% mitochondrial genes its considered apoptotic so we will remove. If a cell has less than 200 genes it likely is not a real cell, i.e. contamination of RNA in the experimental steps, or something is wrong with the cell so we will also remove these.

All human mitochondrial genes start with "MT..." so we can find the percentage of them in each cell using this function
The [[ operator can add columns to object metadata. This is a great place to stash QC stats

```{r}
# making a percent.mt metadata column ad adding percent mitochindiral genes per cell as a value.
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
```

Now we can plot the percentage mitochondrial genes per cell (percent.mt) as well as number of genes per cell (nFeature_RNA) as a violin plot. cut offs are less than 5% mito genes and more than 200 unique genes per cell.

```{r}
# Visualize QC metrics as a violin plot
VlnPlot(pbmc, features = c("nFeature_RNA", "percent.mt"), ncol = 2)
```

now we need to actually remove these cells that do not pass the cut offs

```{r}
pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & percent.mt < 5)
```

Now that we are happy with our cells lets normalise the counts. The best way to do this is using SCTransform for singel cell data but for simplicity we will just use a log transformation.

```{r}
pbmc <- NormalizeData(pbmc)
```

Once we have done a log transformation we then do a linear scaling to shift the expression of each gene to 0 and variance in expression of each gene to 1. we also find the top 2000 variable features (i.e. the 2000 genes with the most variation in gene epxression levels across the cells) which is needed for future steps

```{r}
pbmc <- FindVariableFeatures(pbmc, selection.method = "vst", nfeatures = 2000)

all.genes <- rownames(pbmc)
pbmc <- ScaleData(pbmc, features = all.genes)
```

Now that the data has undergone a linear transformation we are able to perform linear dimensionality reduction by doing Principal component analysis (PCA) on the scaled data. 

```{r}
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
```

Lets visualise the PCA plot now

```{r}
DimPlot(pbmc, reduction = "pca") + NoLegend()
```

Next we should inspect the principal components of the data to only select the top PCs that represent the majority of the variation in the data. This allows us to compress the data set for further dimensionality reduction, only taking the components that seem to represent actual variation in gene expression and not technical or redundant variation between cells. we can visualize this using an elbow plot which plots the standard deviation of each principal component:

```{r}
ElbowPlot(pbmc)
```

It seems that the PC components plateau around PC 10 so we will use PCs 1:10 for further dimensionality reduction. Before this we will run the `FindNeighbors()` function which is a function that, using the euclidean distances in the PCA space we just found, calculates the edge weights between any two cells based on Jaccard similarity (the shared overlap in the local neighborhoods). In layman's terms it just finds which cells are similar (i.e. close together on PCA) and which cells are less similar (further apart) and ranks them as closest together to furthest apart.

Once we have that neighbours value, those values are used to then determine clusters of cells I.e. a gorup of cells that are closer to each other than other cells. the resolution we choose is sort of like a cut off on this clustering where low resolution gives fewer clusters and higher resultions give more clusters.

```{r}
pbmc <- FindNeighbors(pbmc, dims = 1:10)
pbmc <- FindClusters(pbmc, resolution = 0.5)
```

Now that we have our cluster values we should try to visualise them. However, the PCA plot doesnt seem to show the variation well so we should try doing further dimensionality reduction to emphasise these subarchetecture and clustering of or cells. To do this we can run UMAP or uniform manifold approximation and projection. UMAP then takes those PC numbers for each cell and tries to draw a map in 2D space.


```{r}
pbmc <- RunUMAP(pbmc, dims = 1:10)
```

now lets visualise

```{r}
# note that you can set `label = TRUE` or use the LabelClusters function to help label
# individual clusters
DimPlot(pbmc, reduction = "umap")
```

Now Lets use some known cell type markers to try and find what each of these clusters is:

*Markers	--> Cell Type*

IL7R, CCR7 -->	Naive CD4+ T cell
CD14, LYZ	--> CD14+ Monocyte cells
IL7R, S100A4 -->	Memory CD4+ T cell
MS4A1	---> B cells
CD8A	--> CD8+ T cells
FCGR3A, MS4A7 -->	FCGR3A+ Monocyte cells
GNLY, NKG7 -->	NK cells
FCER1A, CST3 -->	Dendritic cells
PPBP -->	Platelets

Lets have a go at visualising expression of these markers in each UMAP cluster

```{r}
VlnPlot(pbmc, features = c("MS4A1", "CD79A"))
```
Now you try for the rest of them.

At the end of the report summarise what cell type each cluster contains. Bonus problem, try to chnage the cluster names and reprin the UMAP with the updated cell type names.


## Statistics Project: Does our new drug have off target effects on blood pressure?

*Scenario & Question*
You’ve just been contracted as a statistical consultant by a public health laboratory in India. The lab has been running a large survey of adults in their city, measuring health indicators like glucose, BMI, insulin, and age. Now, the lab director wants your help:

Brief from the Director:
“We need to understand which of these factors best predict whether someone is diagnosed with diabetes. Please run a proper statistical analysis, give us odds ratios so clinicians can interpret the risks, and check how well the model predicts diabetes using ROC curves and AUC.”

This is your job. Let’s step through the workflow together.

*Step 1: Load the data*

The lab has given us access to a dataset. To simulate their survey, we’ll use the well-known Pima Indians Diabetes dataset from the mlbench package.

```{r}
# Load required packages
#install.packages("mlbench")
library(mlbench)
#install.packages("tidyverse")
library(tidyverse)
library(broom)
#install.packages("pROC")
library(pROC)
#install.packages("caret")
library(caret)
#install.packages("car")
library(car)                 # for VIF
#install.packages("ResourceSelection")
library(ResourceSelection)   # for Hosmer–Lemeshow test

# Load dataset
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2

summary(df)
```

At this stage, think of yourself as opening the lab’s Excel sheet for the first time. You’re scanning it for missing values, strange encodings, or anything suspicious. Notice that missing values are recorded as NA, not zeros, good, that means we don’t need to guess what “0” really means (e.g., is a BMI of 0 realistic?).

*Step 2: Clean the dataset*

We’ll remove rows with missing values in the variables we care about.

```{r}
df_complete <- df %>% drop_na(glucose, insulin, mass, age, diabetes)
summary(df_complete)
```

Why this matters: A clinician cannot interpret results if they come from messy or inconsistent data. In real life, we might impute missing values, but for this workshop we’ll keep it simple and use only complete cases.

*Step 3: Explore the data*

As the lab’s statistician, your first task is to understand the survey population.

```{r}
# Outcome balance
df_complete %>% count(diabetes)

# Distributions by outcome
df_long <- df_complete %>%
  pivot_longer(c(glucose, insulin, mass, age), names_to = "var", values_to = "value")

ggplot(df_long, aes(value, fill = diabetes)) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  facet_wrap(~ var, scales = "free") +
  theme_minimal()

# Bivariate relationships
ggplot(df_complete, aes(glucose, mass, colour = diabetes)) +
  geom_point(alpha = 0.7) +
  theme_minimal()
```

This is like the “first meeting” with your data. You should check:

1. How many people have diabetes vs. how many don’t.

2. Whether glucose, BMI, insulin, or age distributions differ visibly between groups.

3. Outliers: are there extreme values that could distort results? Even if there are should we remove them?

Think of it like being a detective before running the trial in court — you’re gathering evidence before making a claim.

*Step 4: Split into training and testing sets*

As professionals, we must test our model on new data to see if it generalises. That’s why we split the dataset into 70% training (to build the model) and 30% testing (to evaluate it).

```{r}
set.seed(2025)
train_ix <- caret::createDataPartition(df_complete$diabetes, p = 0.7, list = FALSE)
train <- df_complete[train_ix, ]
test  <- df_complete[-train_ix, ]
```

This mimics the real-world scenario. You would first train your statistical model on part of the survey data, then later check if it predicts well on a new batch of patients.

*Step 5: Fit logistic regression*

Now we answer the director’s question: “Which factors predict diabetes?”
Since diabetes is a yes/no outcome, logistic regression is the correct statistical tool.

```{r}
fit <- glm(diabetes ~ age + glucose + mass + insulin,
           data = train, family = binomial)

summary(fit)
```

Here we’re estimating how each predictor changes the odds of having diabetes, while controlling for the others.

*Step 6: Report Odds Ratios (ORs) with Confidence Intervals*

Doctors don’t like log-odds. They want odds ratios e.g. how much the risk changes per unit of glucose/BMI/etc.

```{r}
or_tab <- broom::tidy(fit, conf.int = TRUE, exponentiate = TRUE) %>%
  mutate(across(estimate:conf.high, ~ round(.x, 3))) %>%
  select(term, OR = estimate, `2.5%` = conf.low, `97.5%` = conf.high, p.value)

or_tab
```


Glucose: OR tells us how much the odds of diabetes increase per 1 mg/dL rise.

BMI: OR tells us how much per 1 kg/m² increase.

Age and Insulin: we’ll interpret based on whether they’re significant.

*Step 7: Improve interpretability*

Instead of reporting “per 1 unit increase,” we can scale variables to clinically meaningful increments (e.g., 10 mg/dL of glucose).

```{r}
train_scaled <- train %>%
  mutate(glucose10 = glucose / 10,
         BMI5     = mass / 5)

fit_scaled <- glm(diabetes ~ age + glucose10 + BMI5 + insulin,
                  data = train_scaled, family = binomial)

broom::tidy(fit_scaled, conf.int = TRUE, exponentiate = TRUE)
```

*Step 8: Check multicollinearity*

Doctors may ask: “Are glucose and insulin too correlated to be used together?”
We can check with the Variance Inflation Factor (VIF).

```{r}
car::vif(fit) # values > ~5–10 suggest concern
```

*Step 9: Evaluate predictive performance (ROC/AUC)*

It’s not enough to say “glucose predicts diabetes.” We need to check how well the model discriminates.

```{r}
test$pred_prob <- predict(fit, newdata = test, type = "response")

roc_obj <- pROC::roc(response = test$diabetes,
                     predictor = test$pred_prob,
                     levels = c("neg", "pos"),
                     direction = "<")

auc(roc_obj)
ci.auc(roc_obj)
plot(roc_obj, print.auc = TRUE, legacy.axes = TRUE, main = "ROC — Logistic Model")
```


AUC close to 0.5 → useless model.

AUC close to 1 → excellent discrimination.

Clinicians like to see this curve because it tells them how well your model separates “disease” from “no disease.”

*Step 10: Decision thresholds*

Clinicians need to know when to call someone “at risk.” The ROC curve lets us pick an optimal threshold (often using Youden’s J).

```{r}
opt <- coords(roc_obj, x = "best", best.method = "youden",
              ret = c("threshold", "sensitivity", "specificity"))
opt
```

Here you’re effectively advising the lab: “If you use a probability cut-off of X, you’ll catch Y% of true cases (sensitivity) but risk Z% false positives (1-specificity).

*Step 11: Calibration & model fit*

We also check whether predicted probabilities are well calibrated.

```{r}
hl <- ResourceSelection::hoslem.test(as.numeric(train$diabetes) - 1,
                                     fitted(fit), g = 10)
hl

brier <- mean((as.numeric(test$diabetes) - 1 - test$pred_prob)^2)
brier
```


*Step 12: Presenting results*

Report table for the lab:
```{r}
or_tab %>%
  mutate(p.value = format.pval(p.value, digits = 3)) %>%
  arrange(desc(OR))
```


In our logistic regression analysis, glucose and BMI emerged as strong independent predictors of diabetes. Per 10 mg/dL increase in glucose, the odds of diabetes rose by ~X.X (95% CI A–B). Per 5 kg/m² increase in BMI, the odds increased by ~Y.Y (95% CI C–D). Age and insulin showed weaker effects. The model achieved an AUC of ~Z.Z, indicating good discrimination, with ~S% sensitivity and ~T% specificity at the optimal threshold.

*Step 13: Sensitivity checks* (probably skip)

Try variations to stress-test the model:

Log-transform insulin.

Add a spline for age.

Check glucose × BMI interaction.

*Step 14: Limitations*

Be honest with the lab:

This dataset comes from a specific population; results may not generalise.

Only four predictors — real surveys should include diet, family history, etc.

Measurements (glucose, insulin) are single time points — subject to error.

*Step 15: Deliverables checklist*

As the lab’s contract statistician, here’s what you’ll deliver:

Cleaned data script.

Logistic regression model with ORs & CIs.

ROC curve and AUC.

Short narrative report with interpretation and limitations.

Fully reproducible R Markdown/Quarto file.

# Session Info

```{r}
sessionInfo()
```



